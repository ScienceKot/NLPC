{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\Asus\n",
      "[nltk_data]     VivoBook\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Importing all needed libraries.\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "import os\n",
    "from os.path import join\n",
    "import re\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from functools import reduce\n",
    "from operator import add\n",
    "import numpy as np\n",
    "\n",
    "# Downlaoding the stopwords module.\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the list of stopwords and the Porter Stemmer.\n",
    "stop_words = stopwords.words('english')\n",
    "porter = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The Corpus Reader class.\n",
    "class CorpusReader:\n",
    "    def __init__(self, folder_path, stop_words = None, stemmer = None):\n",
    "        '''\n",
    "            The Corpus Reader constructor\n",
    "                :param folder_path: str\n",
    "                    The path to the folder with files.\n",
    "                :param stop_words: list\n",
    "                    The list o stopwords\n",
    "                :param stemmer: obj\n",
    "                    The stemmer to apply on the text data.\n",
    "        '''\n",
    "        # Setting up the parameters.\n",
    "        self.__folder_path = folder_path\n",
    "        self.__stop_words = stop_words\n",
    "        self.stemmer = stemmer\n",
    "        self.classes_ = os.listdir(self.__folder_path)\n",
    "        self.cls_dict = {}\n",
    "        self.__named_entities = set()\n",
    "        \n",
    "        # Building up the \n",
    "        for cls in self.classes_:\n",
    "            self.cls_dict[cls] = []\n",
    "            for file_path in os.listdir(join(self.__folder_path, cls)):\n",
    "                self.cls_dict[cls].append(join(self.__folder_path, cls, file_path))\n",
    "    \n",
    "    def __getitem__(self, cls):\n",
    "        '''\n",
    "            This function allows accessing the textes in the corpus by name of the class\n",
    "            and the index of the file.\n",
    "                :param cls: tuple\n",
    "                    This argument is passes as a tuple with 2 values:\n",
    "                        1. Name of the class.\n",
    "                        2. Index of the file.\n",
    "        '''\n",
    "        return open(self.cls_dict[cls[0]][cls[1]], 'r', encoding='utf-8').read()\n",
    "    \n",
    "    def __normalize(self, text):\n",
    "        '''\n",
    "            This function normalizes the text.\n",
    "                :param text: str\n",
    "                    The text that should be normalized.\n",
    "        '''\n",
    "        # Leplacing the new line symbols wiht space.\n",
    "        text = text.replace('\\n', ' ')\n",
    "        \n",
    "        # Bringing the text to the lower case.\n",
    "        text = text.lower()\n",
    "        \n",
    "        # Extracting only the words from the text.\n",
    "        text = ' '.join(re.findall('[a-z]+', text))\n",
    "        return text\n",
    "    \n",
    "    def __eliminate_stopwords(self, text):\n",
    "        '''\n",
    "            This function is deleting the stopwords from text.\n",
    "                :param text: str\n",
    "                    The text from which we shoul remove the stopwords.\n",
    "        '''\n",
    "        if self.__stop_words is not None:\n",
    "            return ' '.join([word for word in word_tokenize(text)\n",
    "                            if word not in self.__stop_words])\n",
    "    \n",
    "    def __get_hapaxes(self, dictionary):\n",
    "        '''\n",
    "            This function is responsible for finding the hapaxes.\n",
    "                :param dictionary: dict\n",
    "                    The dixtionary with the all texts separated by classes in the following\n",
    "                    structure:\n",
    "                        {<class> : [<list of textes for this class>]}\n",
    "        '''\n",
    "        # Creatting an empty dictionary for storryng the hapaxes for every class.\n",
    "        self.__hapaxes_per_classes = {}\n",
    "        \n",
    "        # Creatting an empty dictionary for storryn the frequency distribution of words\n",
    "        # for every class\n",
    "        fdist = {}\n",
    "        \n",
    "        # Iterating throw every class.\n",
    "        for cls in self.classes_:\n",
    "            fdist[cls] = FreqDist()\n",
    "            \n",
    "            # Updating the frequenncy distribution of words for the class.\n",
    "            for i in range(len(dictionary[cls])):\n",
    "                fdist[cls].update(FreqDist(word for word in word_tokenize(dictionary[cls][i])))\n",
    "            \n",
    "            # Adding the hapaxes for every class.\n",
    "            self.__hapaxes_per_classes[cls] = fdist[cls].hapaxes()\n",
    "    \n",
    "    def __stem(self, text):\n",
    "        '''\n",
    "            This function amply yhe stemmer on every word in the sentence.\n",
    "                :param text: str\n",
    "                    The text that should be stemmed.\n",
    "        '''\n",
    "        return ' '.join([self.stemmer.stem(word) for word in word_tokenize(text)])\n",
    "    \n",
    "    def __named_identity_extraction(self, text):\n",
    "        '''\n",
    "            This function finds out all named entities in a text and adds it in a set.\n",
    "                :param text: str\n",
    "                    The text from which we should extract the named entities.\n",
    "        '''\n",
    "        for sent in sent_tokenize(text):\n",
    "            for chunk in nltk.ne_chunk(nltk.pos_tag(word_tokenize(sent))):\n",
    "                if hasattr(chunk, 'label'):\n",
    "                    self.__named_entities.add(' '.join(c[0] for c in chunk))\n",
    "    \n",
    "    def process(self):\n",
    "        '''\n",
    "            This function is setting up the corpus reader based on the corpus sent in the \n",
    "            constructor of the reader.\n",
    "        '''\n",
    "        # Creatting a dictionary with all texted from the corpus separated by classes.\n",
    "        text_data = {cls : [] for cls in self.classes_}\n",
    "        for cls in self.classes_:\n",
    "            for i in range(len(os.listdir(join(self.__folder_path, cls)))):\n",
    "                # Loading the corpus in the dictionary.\n",
    "                text_data[cls].append(self.__normalize(self[cls, i]))\n",
    "                \n",
    "                # Adding the names entities extracted for mthe text.\n",
    "                self.__named_identity_extraction(text_data[cls][-1])\n",
    "        \n",
    "        # Gathering all named entities in a general list.\n",
    "        self.named_entities = list(self.__named_entities)\n",
    "        \n",
    "        # Replacing the spaces in named entities wiht underscores.\n",
    "        self.named_entities = [ni.replace(' ', '_') for ni in self.named_entities\n",
    "                              if len(ni) >= 2]\n",
    "        \n",
    "        # Itereting throw every text in the corpus and replacing spaces in named entities\n",
    "        # with underscores.\n",
    "        for cls in self.classes_:\n",
    "            for i in range(len(text_data[cls])):\n",
    "                for ni in self.named_entities:\n",
    "                    text_data[cls][i] = text_data[cls][i].replace(' '.join(\n",
    "                        ni.split('_')), ni)\n",
    "        \n",
    "        # Getting the hapaxes.\n",
    "        self.__get_hapaxes(text_data)\n",
    "        self.common_hapaxes = list(reduce(add, [self.__hapaxes_per_classes[cls]\n",
    "                                               for cls in self.classes_]))\n",
    "        \n",
    "        # Applying the last preprocessing on the text.\n",
    "        for cls in self.classes_:\n",
    "            for i in range(len(text_data[cls])):\n",
    "                # Eliminating the hapaxes.\n",
    "                text_data[cls][i] = ' '.join([word for word in word_tokenize(text_data[cls][i])\n",
    "                                             if word not in self.common_hapaxes])\n",
    "                \n",
    "                # Eliminating the stopwords.\n",
    "                text_data[cls][i] = self.__eliminate_stopwords(text_data[cls][i])\n",
    "                \n",
    "                # Stemming the text.\n",
    "                text_data[cls][i] = self.__stem(text_data[cls][i])\n",
    "                \n",
    "                # Eliminating words with fewer thant 3 letters.\n",
    "                text_data[cls][i] = ' '.join([word for word in word_tokenize(text_data[cls][i])\n",
    "                                             if len(word) >= 3])\n",
    "        \n",
    "        # Generating the X matrix and the y vector for the Machine Learning Pipeline.\n",
    "        X = np.array([text_data[cls][i] for cls in self.classes_ \n",
    "                      for i in range(len(text_data[cls]))])\n",
    "        y = np.array([cls for cls in self.classes_ for i in range(len(text_data[cls]))])\n",
    "        return X, y\n",
    "    \n",
    "    def apply(self, path):\n",
    "        # Creatting a dictionary with all texted from the corpus separated by classes.\n",
    "        text_data = {cls : [] for cls in self.classes_}\n",
    "        \n",
    "        for cls in self.classes_:\n",
    "            # Loading the corpus in the dictionary.\n",
    "            for file_path in os.listdir(join(path, cls)):\n",
    "                text_data[cls].append(self.__normalize(open(\n",
    "                    join(path, cls, file_path), 'r', encoding='utf-8'\n",
    "                ).read()))\n",
    "        \n",
    "        # Itereting throw every text in the corpus and replacing spaces in named entities\n",
    "        # with underscores.\n",
    "        for cls in self.classes_:\n",
    "            for i in range(len(text_data[cls])):\n",
    "                for ni in self.named_entities:\n",
    "                    text_data[cls][i] = text_data[cls][i].replace(' '.join(\n",
    "                        ni.split('_')), ni)\n",
    "        \n",
    "        # Applying the last preprocessing on the text.\n",
    "        for cls in self.classes_:\n",
    "            for i in range(len(text_data[cls])):\n",
    "                # Eliminating the hapaxes.\n",
    "                text_data[cls][i] = ' '.join([word for word in word_tokenize(text_data[cls][i])\n",
    "                                             if word not in self.common_hapaxes])\n",
    "                \n",
    "                # Eliminating the stopwords.\n",
    "                text_data[cls][i] = self.__eliminate_stopwords(text_data[cls][i])\n",
    "                \n",
    "                # Stemming the text.\n",
    "                text_data[cls][i] = self.__stem(text_data[cls][i])\n",
    "                \n",
    "                # Eliminating words with fewer thant 3 letters.\n",
    "                text_data[cls][i] = ' '.join([word for word in word_tokenize(text_data[cls][i])\n",
    "                                             if len(word) >= 3])\n",
    "        \n",
    "        # Generating the X matrix and the y vector for the Machine Learning Pipeline.\n",
    "        X = np.array([text_data[cls][i] for cls in self.classes_ \n",
    "                      for i in range(len(text_data[cls]))])\n",
    "        y = np.array([cls for cls in self.classes_ for i in range(len(text_data[cls]))])\n",
    "        return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creatting the corpus reader.\n",
    "reader = CorpusReader(r'D:\\NLPC\\WS\\BIO_CS_DATA\\TRAIN',stop_words, porter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# processing the training corpus.\n",
    "X_train, y_train = reader.process()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying the changes on the test corpus.\n",
    "X_test, y_test = reader.apply(r'D:\\NLPC\\WS\\BIO_CS_DATA\\TEST')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
